{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def audio_trim(wav, length, threshold=10):\n",
    "    # split an audio signal into non-silent intervals.\n",
    "    non_silence_parts = librosa.effects.split(wav, top_db=threshold)\n",
    "    output = np.concatenate([wav[s:e] for s, e in non_silence_parts])\n",
    "    output = output[1:length+1]\n",
    "    return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wav_file_path = '../audio/wang//test_data/20170001P00235A0066.wav'\n",
    "\n",
    "    wav, _ = librosa.load(wav_file_path, sr=44100)\n",
    "    plt.plot(wav)\n",
    "    plt.show()\n",
    "\n",
    "    output_part = audio_trim(wav, 20000)\n",
    "    print(\"shape\", np.shape(output_part))\n",
    "    plt.plot(output_part)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "\n",
    "def audio_trim(wav, length, threshold=10):\n",
    "    # split an audio signal into non-silent intervals.\n",
    "    non_silence_parts = librosa.effects.split(wav, top_db=threshold)\n",
    "    output = np.concatenate([wav[s:e] for s, e in non_silence_parts])\n",
    "    output = output[1:length+1]\n",
    "    return output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # raw audio file folder path\n",
    "    audio_folder_path = '../audio/'\n",
    "    # mfcc data and labels files path\n",
    "    dataset_file_path = 'dataset.npy'\n",
    "    label_file_path = 'labels.npy'\n",
    "\n",
    "    # get all speakers\n",
    "    speakers = os.listdir(audio_folder_path)\n",
    "    print(\"\\nspeakers:\", speakers, \"\\nlen:\", len(speakers))\n",
    "\n",
    "    dataset = []\n",
    "    labels = []\n",
    "\n",
    "    for speaker in speakers:\n",
    "        cnt = 0\n",
    "        wav_files = os.listdir(audio_folder_path + speaker + '/train_data/')\n",
    "        for wav_file in wav_files:\n",
    "            # print(speaker, 'wav file: ', wav_file)\n",
    "            if '.wav' in wav_file:\n",
    "                # load wav file\n",
    "                wav, sr = librosa.load(path=audio_folder_path + speaker + '/train_data/' + wav_file, sr=16000)\n",
    "                # The actual speaking time is about 2s\n",
    "                wav = audio_trim(wav=wav, length=16384, threshold=30)\n",
    "                # extract mfcc geature\n",
    "                mfcc = librosa.feature.mfcc(y=wav, sr=sr, \n",
    "                                window='hamming', \n",
    "                                win_length=512,\n",
    "                                hop_length=384,\n",
    "                                n_fft=2048,\n",
    "                                n_mels=32,\n",
    "                                n_mfcc=32)\n",
    "                \n",
    "                dataset.append(mfcc)\n",
    "                labels.append(speaker)\n",
    "                cnt += 1\n",
    "\n",
    "                if (cnt % 60 == 0):\n",
    "                    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "                    normalize_mfcc = scaler.fit_transform(mfcc)\n",
    "                    # print(np.square(normalize_mfcc))\n",
    "                    print(speaker, np.shape(mfcc))\n",
    "                    plt.imshow(np.square(normalize_mfcc))\n",
    "                    plt.show()\n",
    "                # print(speaker, cnt, np.shape(mfcc))\n",
    "\n",
    "    # np.save(dataset_file_path, np.array(dataset))\n",
    "    # np.save(label_file_path, np.array(labels))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "\n",
    "def audio_trim(wav, length, threshold=10):\n",
    "    # split an audio signal into non-silent intervals.\n",
    "    non_silence_parts = librosa.effects.split(wav, top_db=threshold)\n",
    "    output = np.concatenate([wav[s:e] for s, e in non_silence_parts])\n",
    "    output = output[1:length+1]\n",
    "    return output\n",
    "\n",
    "\n",
    "def mfcc_normalize(mfcc:np.ndarray,scale:tuple=(0,1)):\n",
    "    ret = mfcc.copy()\n",
    "    min, max = mfcc.min(),mfcc.max()\n",
    "    \n",
    "    ret = (ret - min) / (max - min)\n",
    "    ret = ret * (scale[1] - scale[0]) + scale[0]\n",
    "    return ret\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # raw audio file folder path\n",
    "    audio_folder_path = '../audio/'\n",
    "    # mfcc data and labels files path\n",
    "    dataset_file_path = 'dataset.npy'\n",
    "    label_file_path = 'labels.npy'\n",
    "\n",
    "    # get all speakers\n",
    "    speakers = os.listdir(audio_folder_path)\n",
    "    print(\"\\nspeakers:\", speakers, \"\\nlen:\", len(speakers))\n",
    "\n",
    "    # dataset = []\n",
    "    # labels = []\n",
    "\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    # print all details of large array\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "    for speaker in speakers:\n",
    "        wav_files = os.listdir(audio_folder_path + speaker + '/train_data/')\n",
    "        for wav_file in wav_files:\n",
    "            # print(speaker, 'wav file: ', wav_file)\n",
    "            if '.wav' in wav_file:\n",
    "                # load wav file\n",
    "                wav, sr = librosa.load(path=audio_folder_path + speaker + '/train_data/' + wav_file, sr=44100)\n",
    "                # The actual speaking time is about 2s\n",
    "                wav = audio_trim(wav=wav, length=8192, threshold=30)\n",
    "                # extract mfcc geature (remove the first row)\n",
    "                mfcc = librosa.feature.mfcc(y=wav, sr=sr,\n",
    "                                window='hamming',\n",
    "                                win_length=512,\n",
    "                                hop_length=384,\n",
    "                                n_fft=2048,\n",
    "                                n_mels=8,\n",
    "                                n_mfcc=8)\n",
    "                \n",
    "                \n",
    "                print(speaker, np.shape(mfcc))\n",
    "                # print(mfcc)\n",
    "                # print(normalize_mfcc)\n",
    "                # dataset.append(mfcc)\n",
    "                # labels.append(speaker)\n",
    "                normalize_mfcc = scaler.fit_transform(mfcc)\n",
    "                plt.imshow(np.square(normalize_mfcc))\n",
    "                plt.show()\n",
    "                # print(speaker, cnt, np.shape(mfcc))\n",
    "\n",
    "    # np.save(dataset_file_path, np.array(dataset))\n",
    "    # np.save(label_file_path, np.array(labels))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    a = np.sqrt(np.arange(9).reshape(3,3))\n",
    "    a_n = sklearn.preprocessing.minmax_scale(a)\n",
    "    print(a)\n",
    "    print(np.abs(a_n)) \n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f766cf739c17f21ef0ba6f66b726ee07b7ed00ce2a29ce13d3380a475865bcf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
